{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error = 1.58434144893\n",
      ">=0 and <1: 11640\n",
      ">=1 and <2: 6893\n",
      ">=2 and <3: 2713\n",
      ">=3 and <4: 980\n",
      ">=4: 11127\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "<< Implementing ALS for yelp data set >>\n",
    "1. Filtered the dataset to consider only Tempe businesses\n",
    "2. Created two files, tempe_train and tempe_test using df.randomSplit()\n",
    "3. Refer script (tempe_als_data_prep)\n",
    "4. Trained ALS using tempe_train\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "import collections\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(\"local\", \"Recommendation system\")\n",
    "\n",
    "training_data = sc.textFile(\"tempe_train.csv\")\n",
    "test_data = sc.textFile(\"tempe_test.csv\")\n",
    "\n",
    "#training_data = sc.textFile(sys.argv[1])\n",
    "#test_data = sc.textFile(sys.argv[2])\n",
    "\n",
    "header = training_data.first()\n",
    "header_test = test_data.first()\n",
    "\n",
    "train_rdd = training_data.filter(lambda z: z != header) \\\n",
    "    .map(lambda x: x.split(',')).map(lambda x: (x[0], x[1], x[2]))\n",
    "\n",
    "test_rdd = test_data.filter(lambda z: z != header) \\\n",
    "    .map(lambda x: x.split(',')).map(lambda x: (x[0], x[1], x[2]))\n",
    "\n",
    "all_dist_busId_test = test_rdd.map(lambda x: x[1]).distinct()\n",
    "all_dist_busId_train = train_rdd.map(lambda x: x[1]).distinct()\n",
    "\n",
    "all_dist_busId_final = all_dist_busId_test.union(all_dist_busId_train).distinct()  # .collect()\n",
    "\n",
    "all_busId_dict = {}\n",
    "uniqueId_forAll_busId = all_dist_busId_final.zipWithUniqueId()\n",
    "all_busId_dict = uniqueId_forAll_busId.collectAsMap()\n",
    "\n",
    "##########################################################################################################\n",
    "\n",
    "all_dist_userId_test = test_rdd.map(lambda x: x[0]).distinct()\n",
    "all_dist_userId_train = train_rdd.map(lambda x: x[0]).distinct()\n",
    "\n",
    "all_dist_userId_final = all_dist_userId_test.union(all_dist_userId_train).distinct()\n",
    "\n",
    "all_userId_dict = {}\n",
    "uniqueId_forAll_userId = all_dist_userId_final.zipWithUniqueId()\n",
    "all_userId_dict = uniqueId_forAll_userId.collectAsMap()\n",
    "\n",
    "final_train_rdd2 = train_rdd.map(lambda x:((all_userId_dict[x[0]], all_busId_dict[x[1]], x[2])))\n",
    "\n",
    "final_train_rdd2.collect()\n",
    "\n",
    "final_ratings = final_train_rdd2.map(lambda x: Rating(x[0], x[1], x[2]))\n",
    "\n",
    "rank = 3\n",
    "numIterations = 20\n",
    "\n",
    "model = ALS.train(final_ratings, rank, numIterations, 0.3, seed=300)\n",
    "\n",
    "final_test_rdd = test_rdd.map(lambda x: ((all_userId_dict[x[0]], x[0]), (all_busId_dict[x[1]], x[1]), x[2]))\n",
    "\n",
    "final_test_rdd2 = test_rdd.map(lambda x: (all_userId_dict[x[0]], all_busId_dict[x[1]]))\n",
    "\n",
    "predictions = model.predictAll(final_test_rdd2).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "\n",
    "final_test_rdd = test_rdd.map(lambda x: ((all_userId_dict[x[0]], all_busId_dict[x[1]]), (x[0], x[1], x[2])))\n",
    "\n",
    "ratesAndPreds = final_test_rdd.join(predictions)\n",
    "\n",
    "MSE = ratesAndPreds.map(lambda x: ((float(x[1][0][2]) - x[1][1]) ** 2)).mean()\n",
    "\n",
    "#print(MSE)\n",
    "\n",
    "\n",
    "abs_diff_between_rate_and_pred = ratesAndPreds.map(lambda x: abs(float(x[1][0][2]) - x[1][1]))\n",
    "\n",
    "abs_diff_between_rate_and_pred.take(10)\n",
    "\n",
    "zero_to_one = abs_diff_between_rate_and_pred.filter(lambda x: 0 <= x <= 1.0).count()\n",
    "one_to_two = abs_diff_between_rate_and_pred.filter(lambda x: 1.0 <= x <= 2.0).count()\n",
    "two_to_three = abs_diff_between_rate_and_pred.filter(lambda x: 2.0 <= x <= 3.0).count()\n",
    "three_to_four = abs_diff_between_rate_and_pred.filter(lambda x: 3.0 <= x <= 4.0).count()\n",
    "greater_than_four = abs_diff_between_rate_and_pred.filter(lambda x: x >= 1.0).count()\n",
    "\n",
    "print(\"Root Mean Squared Error = \" + str(math.sqrt(MSE)))\n",
    "print(\">=0 and <1: \" + str(zero_to_one))\n",
    "print(\">=1 and <2: \" + str(one_to_two))\n",
    "print(\">=2 and <3: \" + str(two_to_three))\n",
    "print(\">=3 and <4: \" + str(three_to_four))\n",
    "print(\">=4: \" + str(greater_than_four))\n",
    "\n",
    "final_results_dict = ratesAndPreds.map(lambda x: ((x[1][0][0], x[1][0][1]), x[1][1])).collectAsMap()\n",
    "\n",
    "test_final_results_dict = collections.OrderedDict(sorted(final_results_dict.items()))\n",
    "\n",
    "\n",
    "f = open(\"tempe_ModelBasedCF.txt\", 'w+')\n",
    "\n",
    "for k, v in test_final_results_dict.items():\n",
    "    f.write(str(k[0]) + \",\" + str(k[1]) + \",\" + str(v) + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
