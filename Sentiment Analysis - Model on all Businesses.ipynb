{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions\n",
    "from pyspark.sql.functions import sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"Yelp\").getOrCreate()\n",
    "sqlContext = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = sqlContext.read.json(\"yelp_academic_dataset_review.json\")\n",
    "business_df = sqlContext.read.json(\"yelp_academic_dataset_business.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vigneshwar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Embedding\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import text as txt\n",
    "\n",
    "tk = txt.Tokenizer(split= \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_Reviews = review_df.select(\"business_id\", \"text\", \"stars\")\n",
    "from pyspark.sql.functions import col, asc, when\n",
    "business_Reviews = business_Reviews.orderBy(asc(\"business_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df2 = business_df.withColumn(\"review_count\", business_df[\"review_count\"].cast(\"double\"))\n",
    "business_review_count = business_df2.filter(col('city').isin(['Tempe'])).groupBy(\"business_id\").\\\n",
    "agg(sum(\"review_count\").alias(\"rev_count\"))\n",
    "business_gt5 = business_review_count.filter(\"rev_count >= 5\")\n",
    "right = business_gt5.select(\"business_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempeBusinessReviews = business_Reviews.join(right, \"business_id\")\n",
    "tempeBusinessReviews_1_2_5 = tempeBusinessReviews.filter(\"stars = 1 or stars = 2 or stars = 5\")\n",
    "tempeBusinessReviews_Sentiment = tempeBusinessReviews_1_2_5.\\\n",
    "withColumn(\"sentiment\", when(col(\"stars\") == \"1\", \"0\"). when(col(\"stars\") == \"2\", \"0\"). when(col(\"stars\") == \"5\", \"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+---------+\n",
      "|         business_id|                text|stars|sentiment|\n",
      "+--------------------+--------------------+-----+---------+\n",
      "|--9QQLMTbFzLJ_oT-...|I've been going t...|    5|        1|\n",
      "|--9QQLMTbFzLJ_oT-...|Don't ever believ...|    1|        0|\n",
      "|--9QQLMTbFzLJ_oT-...|Always great cust...|    5|        1|\n",
      "|--9QQLMTbFzLJ_oT-...|Haircut was good ...|    1|        0|\n",
      "|--9QQLMTbFzLJ_oT-...|If you want a man...|    5|        1|\n",
      "+--------------------+--------------------+-----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tempeBusinessReviews_Sentiment.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tempeBusinessReviews_Sentiment.select('text').rdd.map(lambda x: x[0].encode('utf-8').decode()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been going to this particular location for several years.  I always go to the same stylist and she always does a great job.  Someone I can rely on.   I'm not saying who she is because she's busy enough.  I will only say I moved and still go there, even though I'm much farther away and pass many other Great Clips along the way. If you use a coupon then it's really a good price.  But be sure and tip the same as you normally would if you were paying full price.\n"
     ]
    }
   ],
   "source": [
    "tk = txt.Tokenizer(split= \" \")   #tokenizes  \n",
    "print(k[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit_on_texts(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tk.texts_to_sequences(k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tempeBusinessReviews_Sentiment.select('sentiment').rdd.map(lambda x: int(x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000   # the more the better\n",
    "max_length = 500  # cut texts after this number of words (arbitray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sequence.pad_sequences(x, maxlen = max_length, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   81,   132,   559,     1,   229,    12,    38,   728,    80,\n",
       "          18,     5,   338,   897,   164,    73,    17,  1604,    77,\n",
       "           1,  1416,    42,   197,    17, 34768,     2, 19063,    25,\n",
       "          40,    17,   143,     4,   201,    50,     5,  1688,    59,\n",
       "          50,    65,    27,    65,   284, 34769,   310,   115,    42,\n",
       "           1,  2218,   137,     4,  1616,     5,  1610,    10,  4937,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100577 100577 25145 25145\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "print(len(X_train),len(y_train),len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           3200000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           4128      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               2000500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 5,205,129\n",
      "Trainable params: 5,205,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vigneshwar/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=32, kernel_size=4, padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/vigneshwar/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(100000, embedding_vector_length, input_length = max_length))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=4, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=4))       # pooling (max) after convoluting\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))     # relu and sigmoid\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vigneshwar/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100577 samples, validate on 25145 samples\n",
      "Epoch 1/3\n",
      "100577/100577 [==============================] - 186s 2ms/step - loss: 0.1173 - acc: 0.9532 - val_loss: 0.1069 - val_acc: 0.9598\n",
      "Epoch 2/3\n",
      "100577/100577 [==============================] - 179s 2ms/step - loss: 0.0435 - acc: 0.9850 - val_loss: 0.0865 - val_acc: 0.9697\n",
      "Epoch 3/3\n",
      "100577/100577 [==============================] - 169s 2ms/step - loss: 0.0179 - acc: 0.9939 - val_loss: 0.1511 - val_acc: 0.9638\n",
      "Accuracy: 96.38%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=3, batch_size=32, verbose=1)   # batch size powers of two\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"SentimentAnalysisModel\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(\"SentimentAnalysisModel\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.38%\n"
     ]
    }
   ],
   "source": [
    "scores = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
